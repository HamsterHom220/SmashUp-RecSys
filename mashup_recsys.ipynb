{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:18:50.730949400Z",
     "start_time": "2024-06-28T11:18:49.739373300Z"
    }
   },
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "\n",
    "def query_db(query):\n",
    "    try:\n",
    "        cnx = mysql.connector.connect(user='root', password='x155564py',\n",
    "                                  host='127.0.0.1', port=3307,\n",
    "                                  database='smashup')\n",
    "        \n",
    "        if cnx and cnx.is_connected():\n",
    "            with cnx.cursor() as cursor:\n",
    "                cursor.execute(query)\n",
    "                rows = cursor.fetchall()\n",
    "        \n",
    "        cnx.close()\n",
    "        return rows\n",
    "    \n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n",
    "            print(\"Something is wrong with the username or password\")\n",
    "        elif err.errno == errorcode.ER_BAD_DB_ERROR:\n",
    "            print(\"Database does not exist\")\n",
    "        else:\n",
    "            print(err)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c400c896c4bf2c0f",
   "metadata": {},
   "source": [
    "<h1>Content data preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81279306a1d5061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:18:57.437784300Z",
     "start_time": "2024-06-28T11:18:54.134304700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "lim = 10**10\n",
    "def multifeature_encoder(values, ids, num_unique_values=None, num_unique_ids=None):\n",
    "    # inputs - vectors, not ndarrays\n",
    "    values_enum = dict([(i[1],i[0]) for i in enumerate(np.unique(values))])\n",
    "    if num_unique_values is None:\n",
    "        num_unique_values = len(values_enum)\n",
    "    if num_unique_ids is None:\n",
    "        num_unique_ids = len(np.unique(ids))\n",
    "        \n",
    "    result = np.zeros((num_unique_ids, num_unique_values))\n",
    "    prev_id, cur_ind = 0, -1\n",
    "    for v, i in zip(values, ids):\n",
    "        if i!=prev_id:\n",
    "            prev_id = i\n",
    "            cur_ind += 1\n",
    "        result[cur_ind][values_enum[v]] = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2), (0, 3), (0, 4), (1, 5), (0, 6), (1, 7), (1, 8), (0, 9), (0, 10)]\n",
      "[(70507, 1), (213655, 2), (284319, 3), (189495, 4), (49896, 5), (220238, 6), (93413, 7), (99291, 8), (180009, 9), (168000, 10)]\n",
      "[('поп', 1), ('рок', 1), ('рэп', 2), ('электро', 2), ('поп', 3), ('электро', 3), ('поп', 4), ('электро', 4), ('morph', 5), ('поп', 5)]\n"
     ]
    }
   ],
   "source": [
    "print(query_db('SELECT statuses, id FROM mashups LIMIT 10'))\n",
    "print(query_db('SELECT duration, id FROM mashups LIMIT 10'))\n",
    "print(query_db('SELECT genre, mashup_id FROM mashups JOIN mashups_to_genres ON mashups.id=mashups_to_genres.mashup_id LIMIT 10'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T22:45:48.869346700Z",
     "start_time": "2024-06-26T22:45:48.773233200Z"
    }
   },
   "id": "a58eda2a692de5ce",
   "execution_count": 386
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8cd414721efdbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:19:04.287739400Z",
     "start_time": "2024-06-28T11:19:04.045511900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778, 1) (778, 1) (1296, 2)\n",
      "(778, 14)\n",
      "(778, 16)\n",
      "[[0.00000e+00 7.05070e+04 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [1.00000e+00 2.13655e+05 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00]\n",
      " [0.00000e+00 2.84319e+05 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00]\n",
      " ...\n",
      " [0.00000e+00 1.71075e+05 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00]\n",
      " [1.00000e+00 1.51405e+05 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [0.00000e+00 2.76662e+05 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# vector for each feature (maybe this should be a transaction for reading consistency!)\n",
    "ids = np.array(query_db(f'SELECT id FROM mashups LIMIT {lim}'))\n",
    "id_lim = max(ids)[0]\n",
    "\n",
    "statuses = np.array(query_db(f'SELECT statuses FROM mashups WHERE id<{id_lim}'))\n",
    "durations = np.array(query_db(f'SELECT duration FROM mashups WHERE id<{id_lim}'))\n",
    "genres_raw = np.array(query_db(f'SELECT genre, mashup_id FROM mashups JOIN mashups_to_genres ON mashups.id=mashups_to_genres.mashup_id WHERE mashup_id<{id_lim}'))\n",
    "\n",
    "print(np.shape(statuses),np.shape(durations),np.shape(genres_raw))\n",
    "\n",
    "n_unique_values = int(query_db('SELECT COUNT(DISTINCT genre) FROM mashups_to_genres')[0][0])\n",
    "n_unique_ids = int(query_db(f'SELECT COUNT(id) FROM mashups WHERE id<{id_lim}')[0][0])\n",
    "genres = multifeature_encoder(genres_raw[:,0],genres_raw[:,1],n_unique_values,n_unique_ids)\n",
    "print(np.shape(genres))\n",
    "\n",
    "features = (statuses, durations, genres)\n",
    "for f in features:\n",
    "    f[np.isnan(f)] = 0\n",
    "\n",
    "X = np.hstack(features)\n",
    "print(np.shape(X))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa7b95df4f9dfc8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:19:07.466010100Z",
     "start_time": "2024-06-28T11:19:07.450496300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.96712118 -1.1728117  -0.13035898 ... -0.13035898 -0.15821801\n",
      "  -0.75402148]\n",
      " [ 1.03399658  0.7447916  -0.13035898 ... -0.13035898 -0.15821801\n",
      "   1.32622217]\n",
      " [-0.96712118  1.69140294 -0.13035898 ... -0.13035898 -0.15821801\n",
      "   1.32622217]\n",
      " ...\n",
      " [-0.96712118  0.1743921  -0.13035898 ... -0.13035898 -0.15821801\n",
      "   1.32622217]\n",
      " [ 1.03399658 -0.08910622 -0.13035898 ... -0.13035898 -0.15821801\n",
      "  -0.75402148]\n",
      " [-0.96712118  1.58883016 -0.13035898 ... -0.13035898 -0.15821801\n",
      "  -0.75402148]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c8635bd403949",
   "metadata": {},
   "source": [
    "<h1>Content clustering (optional)</h1> \n",
    "for better scalability: enables search over just one corresponding cluster instead of all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "35e919c9f383dc83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T22:45:49.131682100Z",
     "start_time": "2024-06-26T22:45:49.073208900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 8\n",
      "Estimated number of noise points: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "# estimate the number of clusters\n",
    "dbscan = DBSCAN(eps=0.35, min_samples=5, metric='cosine').fit(X_scaled)\n",
    "labels = dbscan.labels_\n",
    "\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "b52d128dc3ae38f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T22:45:49.165229600Z",
     "start_time": "2024-06-26T22:45:49.115656300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778,)\n",
      "[4 2 1 1 1 4 4 2 5 5 5 5 2 2 1 1 2 2 2 1 1 2 1 2 2 4 2 4 2 2 2 1 2 2 6 2 2\n",
      " 1 2 1 1 1 6 4 6 6 1 4 6 4 1 4 2 2 2 1 2 2 2 2 4 2 6 6 6 6 6 6 6 6 6 6 0 2\n",
      " 4 1 1 1 2 2 2 2 2 6 6 6 2 1 2 6 4 4 1 4 6 6 6 6 6 2 6 6 6 2 2 2 2 4 1 1 1\n",
      " 1 1 1 2 1 6 2 2 2 6 2 2 1 1 4 1 6 4 6 6 6 4 4 1 6 4 2 0 2 2 2 2 2 2 6 1 1\n",
      " 1 2 1 6 1 1 2 2 2 1 1 6 6 1 6 1 1 1 4 2 1 4 6 6 6 6 4 6 2 4 4 6 1 6 6 4 2\n",
      " 2 6 2 6 6 4 2 2 1 2 6 1 1 4 4 4 2 2 4 2 2 6 4 1 1 1 1 6 6 4 1 2 1 4 2 2 6\n",
      " 1 4 6 2 2 6 6 6 6 2 1 6 6 6 3 4 6 4 2 1 2 1 2 1 4 1 3 1 1 2 2 4 6 1 1 4 2\n",
      " 1 6 6 1 4 1 4 4 1 4 4 0 2 2 2 2 2 2 2 2 2 2 2 2 3 1 1 4 3 4 6 6 0 2 2 1 4\n",
      " 2 4 1 1 4 4 4 4 4 4 1 3 4 1 3 3 1 2 1 4 1 2 4 1 2 1 4 6 4 4 6 6 2 6 6 4 2\n",
      " 2 2 6 6 2 3 4 6 2 4 2 2 6 6 6 1 6 2 2 2 2 7 1 1 4 2 4 1 1 1 1 4 4 4 1 1 1\n",
      " 1 2 2 2 2 1 1 4 2 4 4 4 4 7 2 4 4 2 2 4 2 4 2 1 2 4 4 4 2 6 2 7 2 2 2 2 2\n",
      " 2 2 2 2 4 4 0 2 1 7 7 7 2 6 2 2 2 2 2 1 2 6 4 7 4 6 2 2 2 6 2 6 6 6 2 6 2\n",
      " 2 2 2 2 2 4 1 1 2 3 2 7 1 2 3 0 4 1 1 4 1 2 4 4 2 2 1 1 4 1 4 4 1 4 4 4 4\n",
      " 4 1 2 3 2 1 2 6 2 2 6 1 6 2 1 2 6 1 2 7 5 4 2 1 2 1 1 0 4 4 6 2 2 6 6 6 1\n",
      " 6 1 6 1 2 2 2 2 4 4 6 6 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 1 2 2 1 1 4 2 2 2 2 6 4 2 1 2 2 1 1 1 2 2 2 2 7 1 2 7 7 1 2 2 3 1 2 4 6\n",
      " 6 1 4 1 2 7 2 2 2 2 4 2 2 1 6 6 2 1 4 1 5 1 6 6 4 2 6 2 4 2 1 1 2 1 1 2 2\n",
      " 1 2 1 2 2 1 6 2 2 2 2 2 1 1 1 1 1 5 2 1 4 5 2 2 4 5 6 2 1 6 6 2 2 1 4 2 6\n",
      " 1 2 1 2 6 6 1 1 1 4 1 2 2 4 1 5 1 4 4 1 2 5 0 5 2 4 1 2 1 4 1 2 6 2 2 4 1\n",
      " 5 2 2 2 2 1 6 6 1 1 1 1 6 1 4 6 2 1 0 2 1 0 0 1 2 1 1 4 2 2 0 0 0 0 0 0 0\n",
      " 0 0 2 0 2 1 1 1 3 4 4 2 2 1 1 1 2 6 2 4 2 6 4 3 1 3 2 3 4 1 2 2 1 3 2 4 2\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# using estimated K, apply K-means clustering for convenient prediction of cluster for new data\n",
    "kmeans = KMeans(n_clusters=n_clusters_).fit(X_scaled)\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "print(np.shape(labels))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "2e8799a6291b493f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T22:45:49.189641800Z",
     "start_time": "2024-06-26T22:45:49.165229600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "new_mashup = np.array([ 1.03528185, -0.08797488, -0.13527991, -0.13027386, \n",
    "                        -0.15811388, -0.75326252, 0.08797488, - 1.03528185,\n",
    "                        1.03528185, -0.08797488, -0.13527991, -0.13027386, \n",
    "                        -0.15811388, -0.75326252, 0.08797488, - 1.03528185],ndmin=2)\n",
    "new_mashup = scaler.transform(new_mashup)\n",
    "print(kmeans.predict(new_mashup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "a8d698f07ce2ecbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T22:45:49.228575200Z",
     "start_time": "2024-06-26T22:45:49.177420700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16)\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "duplicate_mashup = np.array(X_scaled[8,:], ndmin=2)\n",
    "print(np.shape(duplicate_mashup))\n",
    "print(kmeans.predict(duplicate_mashup))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25d314235ade56",
   "metadata": {},
   "source": [
    "For further application, additional datastructures or table in the database are needed for storage of pairs \"mashup id - cluster label\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb246f5df233d6f",
   "metadata": {},
   "source": [
    "<h1>Content Filtering: Candidate selection</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f69ce57a4fd05d46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:19:14.161077600Z",
     "start_time": "2024-06-28T11:19:14.140993Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_content_data_point(mashup_id):\n",
    "    status = np.array(query_db(f'SELECT statuses FROM mashups WHERE id={mashup_id}'))\n",
    "    duration = np.array(query_db(f'SELECT duration FROM mashups WHERE id={mashup_id}'))\n",
    "    genre_raw = np.array(query_db(f'SELECT genre FROM mashups JOIN mashups_to_genres ON mashups.id=mashups_to_genres.mashup_id WHERE mashup_id={mashup_id}'))\n",
    "    \n",
    "    n_unique_values = int(query_db('SELECT COUNT(DISTINCT genre) FROM mashups_to_genres')[0][0])\n",
    "    genre = multifeature_encoder(genre_raw.reshape(1,-1)[0], np.array([mashup_id]*len(genre_raw)), n_unique_values)\n",
    "    features = (status, duration, genre)\n",
    "    for f in features:\n",
    "        f[np.isnan(f)] = 0\n",
    "    \n",
    "    # each datapoint is of shape (1,16)\n",
    "    x = np.hstack(features)\n",
    "    return scaler.transform(x)\n",
    "\n",
    "def get_pop_ids(user_id, liked_population_size=3, most_listened_population_size=3, recently_listened_population_size=3):\n",
    "    liked_population = query_db(f'SELECT mashup_id FROM mashups_likes WHERE user_id={user_id} LIMIT {liked_population_size}')\n",
    "    most_listened_population = query_db(f\"SELECT mashup_id FROM mashups_likes WHERE user_id={user_id} GROUP BY mashup_id ORDER BY COUNT(time) DESC LIMIT {most_listened_population_size}\")\n",
    "    recently_listened_population = query_db(f\"SELECT mashup_id FROM mashups_likes WHERE user_id={user_id} ORDER BY time DESC LIMIT {recently_listened_population_size}\")\n",
    "    return liked_population, most_listened_population, recently_listened_population\n",
    "\n",
    "def get_pop_data(liked_population_ids, most_listened_population_ids, recently_listened_population_ids):\n",
    "    liked_population = np.zeros((len(liked_population_ids),16))\n",
    "    most_listened_population = np.zeros((len(most_listened_population_ids),16))\n",
    "    recently_listened_population = np.zeros((len(recently_listened_population_ids), 16))\n",
    "    \n",
    "    # each datapoint is of shape (1,16)\n",
    "    for i in range(len(liked_population_ids)):\n",
    "        liked_population[i] = get_content_data_point(liked_population_ids[i][0])\n",
    "\n",
    "    for i in range(len(most_listened_population_ids)):\n",
    "        most_listened_population[i] = get_content_data_point(most_listened_population_ids[i][0])\n",
    "\n",
    "    for i in range(len(recently_listened_population_ids)):\n",
    "        recently_listened_population[i] = get_content_data_point(recently_listened_population_ids[i][0])\n",
    "    \n",
    "    # liked_population = np.array(liked_population).reshape(3,16)\n",
    "    # most_listened_population = np.array(most_listened_population).reshape(3,16)\n",
    "    # recently_listened_population = np.array(recently_listened_population).reshape(3,16)\n",
    "    return liked_population, most_listened_population, recently_listened_population\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def filter_already_liked(user_id, mashup_ids):\n",
    "    likes = [i[0] for i in query_db(f'SELECT mashup_id FROM mashups_likes WHERE user_id={user_id}')]\n",
    "    filtered_ids = []\n",
    "    for i in mashup_ids:\n",
    "        if i not in likes:\n",
    "            filtered_ids.append(i)\n",
    "    return np.array(filtered_ids)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T11:25:34.048145Z",
     "start_time": "2024-06-28T11:25:34.029081500Z"
    }
   },
   "id": "7022194b3a1e4dff",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9b9324498a48c83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T22:24:42.525972400Z",
     "start_time": "2024-06-28T22:24:42.487038900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "knn = NearestNeighbors().fit(X_scaled)\n",
    "\n",
    "def select_base_candidates(liked_population, most_listened_population, recently_listened_population, l_neighbors=5, m_neighbors=5, r_neighbors=5):\n",
    "    # neighbor search (candidates are returned as indices stored in ids and corresponding to elements of X provided to the KNN)\n",
    "    l_dist, l_candidates = knn.kneighbors(liked_population, l_neighbors)\n",
    "    m_dist, m_candidates = knn.kneighbors(most_listened_population, m_neighbors)\n",
    "    r_dist, r_candidates = knn.kneighbors(recently_listened_population, r_neighbors)\n",
    "\n",
    "    print(np.shape(l_dist), np.shape(m_dist), np.shape(r_dist)) # (n_population, n_neighbors)\n",
    "    # return l_dist, m_dist, r_dist, l_candidates, m_candidates, r_candidates\n",
    "    return np.concatenate((l_candidates.flatten(),m_candidates.flatten(),r_candidates.flatten()), axis=None)\n",
    "\n",
    "def select_playlist_candidates(user_id, liked_population_ids, most_listened_population_ids, recently_listened_population_ids, n_neighbors=5, lim=3):\n",
    "    # neighbor search based on playlist data (mashup-candidates are returned as indices stored in ids and corresponding to elements of X provided to the KNN)\n",
    "    playlist_ids = set()\n",
    "    # playlists including songs from the populations\n",
    "    for pop in [liked_population_ids, most_listened_population_ids, recently_listened_population_ids]:\n",
    "        for i in tqdm(pop):\n",
    "            for j in query_db(f'SELECT playlist_id FROM playlists_to_mashups WHERE mashup_id = {i[0]}'):\n",
    "                playlist_ids.add(j[0])\n",
    "    \n",
    "    # liked playlists\n",
    "    for i in query_db(f'SELECT playlist_id FROM playlists_likes WHERE user_id = {user_id}'):\n",
    "        playlist_ids.add(i[0])\n",
    "    \n",
    "    candidates = []\n",
    "    for pi in tqdm(playlist_ids):\n",
    "        mashup_ids = query_db(f'SELECT mashup_id FROM playlists_to_mashups WHERE playlist_id = {pi}')\n",
    "        for mi in mashup_ids:\n",
    "            mashup_data = get_content_data_point(mi[0])\n",
    "            dist, neigh_inds = knn.kneighbors(mashup_data, n_neighbors)\n",
    "            for i in range(n_neighbors):\n",
    "                candidates.append((dist[0][i], neigh_inds[0][i]))\n",
    "                \n",
    "    return set([pair[1] for pair in sorted(candidates, key=lambda pair: pair[0])][:lim])\n",
    "\n",
    "def select_author_candidates(liked_population_ids, most_listened_population_ids, recently_listened_population_ids, n_neighbors=5, lim=2):\n",
    "    # neighbor search based on mashup author data (mashup-candidates are returned as indices stored in ids and corresponding to elements of X provided to the KNN)\n",
    "    author_ids = set()\n",
    "    # authors of mashups from the populations\n",
    "    for pop in [liked_population_ids, most_listened_population_ids, recently_listened_population_ids]:\n",
    "        for i in tqdm(pop):\n",
    "            for j in query_db(f'SELECT user_id FROM mashups_to_authors WHERE mashup_id={i[0]}'):\n",
    "                author_ids.add(j[0])\n",
    "            \n",
    "    # liked mashups - already in liked_population\n",
    "            \n",
    "    candidates = []\n",
    "    for ai in tqdm(author_ids): # very slow\n",
    "        mashup_ids = query_db(f'SELECT mashup_id FROM mashups_to_authors WHERE user_id={ai}')\n",
    "        for mi in mashup_ids:\n",
    "            mashup_data = get_content_data_point(mi[0])\n",
    "            dist, neigh_inds = knn.kneighbors(mashup_data, n_neighbors)\n",
    "            for i in range(n_neighbors):\n",
    "                candidates.append((dist[0][i], neigh_inds[0][i]))\n",
    "                \n",
    "    return set([pair[1] for pair in sorted(candidates, key=lambda pair: pair[0])][:lim])\n",
    "        \n",
    "def select_track_candidates(liked_population_ids, most_listened_population_ids, recently_listened_population_ids, n_neighbors=5, lim=3):\n",
    "    # neighbor search based on data about tracks that the mashup consists of (mashup-candidates are returned as indices stored in ids and corresponding to elements of X provided to the KNN)\n",
    "    track_ids = set()\n",
    "    # get tracks from mashups from the populations (liked mashups - already in liked_population)\n",
    "    for pop in [liked_population_ids, most_listened_population_ids, recently_listened_population_ids]:\n",
    "        for i in tqdm(pop):\n",
    "            for j in query_db(f'SELECT track_id FROM mashups_to_tracks WHERE mashup_id={i[0]}'):\n",
    "                track_ids.add(j[0])\n",
    "            \n",
    "    # get the tracks' authors\n",
    "    author_ids = set()\n",
    "    for i in track_ids:\n",
    "        for j in query_db(f'SELECT author_id FROM tracks_to_authors WHERE track_id={i}'):\n",
    "            author_ids.add(j[0])\n",
    "        \n",
    "    # get other tracks of the authors\n",
    "    for i in author_ids:\n",
    "        for j in query_db(f'SELECT track_id FROM tracks_to_authors WHERE author_id={i}'):\n",
    "            track_ids.add(j[0])\n",
    "    \n",
    "    # recommend other mashups that include these tracks\n",
    "    candidates = []\n",
    "    for ti in tqdm(track_ids):\n",
    "        mashup_ids = query_db(f'SELECT mashup_id FROM mashups_to_tracks WHERE track_id={ti}')\n",
    "        for mi in mashup_ids:\n",
    "            mashup_data = get_content_data_point(mi[0])\n",
    "            dist, neigh_inds = knn.kneighbors(mashup_data, n_neighbors)\n",
    "            for i in range(n_neighbors):\n",
    "                candidates.append((dist[0][i], neigh_inds[0][i]))\\\n",
    "            \n",
    "    return set([pair[1] for pair in sorted(candidates, key=lambda pair: pair[0])][:lim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcb419dfd165ec2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T22:26:16.761307800Z",
     "start_time": "2024-06-28T22:24:44.280682400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5) (3, 5) (3, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 25.41it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 34.39it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 32.79it/s]\n",
      "100%|██████████| 15/15 [00:46<00:00,  3.13s/it]\n",
      "100%|██████████| 3/3 [00:00<00:00, 27.91it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 24.38it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 38.85it/s]\n",
      "100%|██████████| 9/9 [00:12<00:00,  1.34s/it]\n",
      "100%|██████████| 3/3 [00:00<00:00, 26.61it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 30.47it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 32.51it/s]\n",
      "100%|██████████| 325/325 [00:30<00:00, 10.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,) [689 650 681 687 654 364 365 325 136 138 654 687 646 681 650 646 654 687\n",
      " 681 650 646 654 687 681 650 681 650 689 687 654 681 687 650 654 689 689\n",
      " 650 681 687 654 689 650 681 687 654]\n",
      "() {681, 689, 687}\n",
      "() {650, 646}\n",
      "() {681, 650, 689}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "USER_ID = 2\n",
    "population_ids = get_pop_ids(USER_ID)\n",
    "populations = get_pop_data(*population_ids)\n",
    "base_cand = select_base_candidates(*populations)\n",
    "playlist_cand = select_playlist_candidates(USER_ID, *population_ids)\n",
    "author_cand = select_author_candidates(*population_ids)\n",
    "track_cand = select_track_candidates(*population_ids)\n",
    "\n",
    "print(np.shape(base_cand), base_cand)\n",
    "print(np.shape(playlist_cand), playlist_cand)\n",
    "print(np.shape(author_cand), author_cand)\n",
    "print(np.shape(track_cand), track_cand)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n"
     ]
    }
   ],
   "source": [
    "total_candidates = ..."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T22:45:50.338266800Z",
     "start_time": "2024-06-26T22:45:50.309710400Z"
    }
   },
   "id": "5700c17364458137",
   "execution_count": 398
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Collaborative Filtering (TODO)</h1>\n",
    "Find users that liked the songs among total_candidates.<br>\n",
    "If there is not enough such users ( < num_users), then find users that listened these songs more than once.<br>\n",
    "If still not enough, select randomly.<br>\n",
    "In fact, this is an heuristic instead of KNN, which would include measuring the similarity of a given user and each other user."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d245c010f8c2d422"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_collab_users = 10\n",
    "num_collab_recs = 15"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T22:45:50.356782900Z",
     "start_time": "2024-06-26T22:45:50.326253Z"
    }
   },
   "id": "9e45cb00b37fa1a6",
   "execution_count": 399
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1802,), (2,), (2259,), (1257,), (2321,), (1721,), (2548,), (2732,), (2705,)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Ellipsis"
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collab_users = []\n",
    "for ind in total_candidates:\n",
    "    candidate_id = ids[ind][0]\n",
    "    response = query_db(f'SELECT user_id FROM mashups_likes WHERE mashup_id={candidate_id} LIMIT {num_collab_users}')\n",
    "    print(response)\n",
    "    break\n",
    "    \n",
    "..."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T22:51:30.140422200Z",
     "start_time": "2024-06-26T22:51:30.107272200Z"
    }
   },
   "id": "39a65ff5161fd5f6",
   "execution_count": 403
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suggest to the given user such songs that they haven't listened yet, but similar users liked."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1143a550e1bd061c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Ellipsis"
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T22:45:50.415720400Z",
     "start_time": "2024-06-26T22:45:50.387396100Z"
    }
   },
   "id": "a28a14e4c7922840",
   "execution_count": 401
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
