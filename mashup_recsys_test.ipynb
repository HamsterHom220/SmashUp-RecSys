{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T11:10:32.837316600Z",
     "start_time": "2024-07-02T11:10:32.790624100Z"
    }
   },
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "\n",
    "def query_db(query):\n",
    "    try:\n",
    "        cnx = mysql.connector.connect(user='root', password='x155564py',\n",
    "                                  host='127.0.0.1', port=3307,\n",
    "                                  database='smashup')\n",
    "        \n",
    "        if cnx and cnx.is_connected():\n",
    "            with cnx.cursor() as cursor:\n",
    "                cursor.execute(query)\n",
    "                rows = cursor.fetchall()\n",
    "        \n",
    "        cnx.close()\n",
    "        return rows\n",
    "    \n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n",
    "            print(\"Something is wrong with the username or password\")\n",
    "        elif err.errno == errorcode.ER_BAD_DB_ERROR:\n",
    "            print(\"Database does not exist\")\n",
    "        else:\n",
    "            print(err)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c400c896c4bf2c0f",
   "metadata": {},
   "source": [
    "<h1>Content data preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f81279306a1d5061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T11:10:32.838328400Z",
     "start_time": "2024-07-02T11:10:32.809180500Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "lim = 10**10\n",
    "def multifeature_encoder(values, ids, num_unique_values=None, num_unique_ids=None):\n",
    "    # inputs - vectors, not ndarrays\n",
    "    values_enum = dict([(i[1],i[0]) for i in enumerate(np.unique(values))])\n",
    "    if num_unique_values is None:\n",
    "        num_unique_values = len(values_enum)\n",
    "    if num_unique_ids is None:\n",
    "        num_unique_ids = len(np.unique(ids))\n",
    "        \n",
    "    result = np.zeros((num_unique_ids, num_unique_values))\n",
    "    prev_id, cur_ind = 0, -1\n",
    "    for v, i in zip(values, ids):\n",
    "        if i!=prev_id:\n",
    "            prev_id = i\n",
    "            cur_ind += 1\n",
    "        result[cur_ind][values_enum[v]] = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2), (0, 3), (0, 4), (1, 5), (0, 6), (1, 7), (1, 8), (0, 9), (0, 10)]\n",
      "[(70507, 1), (213655, 2), (284319, 3), (189495, 4), (49896, 5), (220238, 6), (93413, 7), (99291, 8), (180009, 9), (168000, 10)]\n",
      "[('поп', 1), ('рок', 1), ('рэп', 2), ('электро', 2), ('поп', 3), ('электро', 3), ('поп', 4), ('электро', 4), ('morph', 5), ('поп', 5)]\n"
     ]
    }
   ],
   "source": [
    "print(query_db('SELECT statuses, id FROM mashups LIMIT 10'))\n",
    "print(query_db('SELECT duration, id FROM mashups LIMIT 10'))\n",
    "print(query_db('SELECT genre, mashup_id FROM mashups JOIN mashups_to_genres ON mashups.id=mashups_to_genres.mashup_id LIMIT 10'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T11:10:32.941626100Z",
     "start_time": "2024-07-02T11:10:32.840830400Z"
    }
   },
   "id": "a58eda2a692de5ce",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8cd414721efdbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T11:10:33.089070Z",
     "start_time": "2024-07-02T11:10:32.921575700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778, 1) (778, 1) (1296, 2)\n",
      "(778, 14)\n",
      "(778, 16)\n",
      "[[0.00000e+00 7.05070e+04 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [1.00000e+00 2.13655e+05 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00]\n",
      " [0.00000e+00 2.84319e+05 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00]\n",
      " ...\n",
      " [0.00000e+00 1.71075e+05 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00]\n",
      " [1.00000e+00 1.51405e+05 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [0.00000e+00 2.76662e+05 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# vector for each feature (maybe this should be a transaction for reading consistency!)\n",
    "ids = np.array(query_db(f'SELECT id FROM mashups LIMIT {lim}'))\n",
    "id_lim = max(ids)[0]\n",
    "\n",
    "statuses = np.array(query_db(f'SELECT statuses FROM mashups WHERE id<{id_lim}'))\n",
    "durations = np.array(query_db(f'SELECT duration FROM mashups WHERE id<{id_lim}'))\n",
    "genres_raw = np.array(query_db(f'SELECT genre, mashup_id FROM mashups JOIN mashups_to_genres ON mashups.id=mashups_to_genres.mashup_id WHERE mashup_id<{id_lim}'))\n",
    "\n",
    "print(np.shape(statuses),np.shape(durations),np.shape(genres_raw))\n",
    "\n",
    "n_unique_values = int(query_db('SELECT COUNT(DISTINCT genre) FROM mashups_to_genres')[0][0])\n",
    "n_unique_ids = int(query_db(f'SELECT COUNT(id) FROM mashups WHERE id<{id_lim}')[0][0])\n",
    "genres = multifeature_encoder(genres_raw[:,0],genres_raw[:,1],n_unique_values,n_unique_ids)\n",
    "print(np.shape(genres))\n",
    "\n",
    "features = (statuses, durations, genres)\n",
    "for f in features:\n",
    "    f[np.isnan(f)] = 0\n",
    "\n",
    "X = np.hstack(features)\n",
    "print(np.shape(X))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa7b95df4f9dfc8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T11:10:33.132738600Z",
     "start_time": "2024-07-02T11:10:33.091589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.96712118 -1.1728117  -0.13035898 ... -0.13035898 -0.15821801\n",
      "  -0.75402148]\n",
      " [ 1.03399658  0.7447916  -0.13035898 ... -0.13035898 -0.15821801\n",
      "   1.32622217]\n",
      " [-0.96712118  1.69140294 -0.13035898 ... -0.13035898 -0.15821801\n",
      "   1.32622217]\n",
      " ...\n",
      " [-0.96712118  0.1743921  -0.13035898 ... -0.13035898 -0.15821801\n",
      "   1.32622217]\n",
      " [ 1.03399658 -0.08910622 -0.13035898 ... -0.13035898 -0.15821801\n",
      "  -0.75402148]\n",
      " [-0.96712118  1.58883016 -0.13035898 ... -0.13035898 -0.15821801\n",
      "  -0.75402148]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c8635bd403949",
   "metadata": {},
   "source": [
    "<h1>Content clustering (optional)</h1> \n",
    "for better scalability: enables search over just one corresponding cluster instead of all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35e919c9f383dc83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T11:10:33.133731800Z",
     "start_time": "2024-07-02T11:10:33.109106400Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans, DBSCAN\n",
    "# \n",
    "# # estimate the number of clusters\n",
    "# dbscan = DBSCAN(eps=0.35, min_samples=5, metric='cosine').fit(X_scaled)\n",
    "# labels = dbscan.labels_\n",
    "# \n",
    "# n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "# n_noise_ = list(labels).count(-1)\n",
    "# \n",
    "# print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "# print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b52d128dc3ae38f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T11:10:33.159773300Z",
     "start_time": "2024-07-02T11:10:33.125661800Z"
    }
   },
   "outputs": [],
   "source": [
    "# # using estimated K, apply K-means clustering for convenient prediction of cluster for new data\n",
    "# kmeans = KMeans(n_clusters=n_clusters_).fit(X_scaled)\n",
    "# labels = kmeans.labels_\n",
    "# centers = kmeans.cluster_centers_\n",
    "# print(np.shape(labels))\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e8799a6291b493f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T11:10:33.161294300Z",
     "start_time": "2024-07-02T11:10:33.141240700Z"
    }
   },
   "outputs": [],
   "source": [
    "# new_mashup = np.array([ 1.03528185, -0.08797488, -0.13527991, -0.13027386, \n",
    "#                         -0.15811388, -0.75326252, 0.08797488, - 1.03528185,\n",
    "#                         1.03528185, -0.08797488, -0.13527991, -0.13027386, \n",
    "#                         -0.15811388, -0.75326252, 0.08797488, - 1.03528185],ndmin=2)\n",
    "# new_mashup = scaler.transform(new_mashup)\n",
    "# print(kmeans.predict(new_mashup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a8d698f07ce2ecbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T11:10:33.171778400Z",
     "start_time": "2024-07-02T11:10:33.155775900Z"
    }
   },
   "outputs": [],
   "source": [
    "# duplicate_mashup = np.array(X_scaled[8,:], ndmin=2)\n",
    "# print(np.shape(duplicate_mashup))\n",
    "# print(kmeans.predict(duplicate_mashup))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25d314235ade56",
   "metadata": {},
   "source": [
    "For further application, additional datastructures or table in the database are needed for storage of pairs \"mashup id - cluster label\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb246f5df233d6f",
   "metadata": {},
   "source": [
    "<h1>Content Filtering: Candidate selection</h1>"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def filter_already_liked(user_id, mashup_ids):\n",
    "    likes = [i[0] for i in query_db(f'SELECT mashup_id FROM mashups_likes WHERE user_id={user_id}')]\n",
    "    filtered_ids = []\n",
    "    for i in mashup_ids:\n",
    "        if i not in likes:\n",
    "            filtered_ids.append(i)\n",
    "    return filtered_ids\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T11:10:33.191814300Z",
     "start_time": "2024-07-02T11:10:33.171778400Z"
    }
   },
   "id": "7022194b3a1e4dff",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_content_data_point(mashup_id):\n",
    "    status = np.array(query_db(f'SELECT statuses FROM mashups WHERE id={mashup_id}'))\n",
    "    duration = np.array(query_db(f'SELECT duration FROM mashups WHERE id={mashup_id}'))\n",
    "    genre_raw = np.array(query_db(f'SELECT genre FROM mashups JOIN mashups_to_genres ON mashups.id=mashups_to_genres.mashup_id WHERE mashup_id={mashup_id}'))\n",
    "    \n",
    "    n_unique_values = int(query_db('SELECT COUNT(DISTINCT genre) FROM mashups_to_genres')[0][0])\n",
    "    genre = multifeature_encoder(genre_raw.reshape(1,-1)[0], np.array([mashup_id]*len(genre_raw)), n_unique_values)\n",
    "    features = (status, duration, genre)\n",
    "    for f in features:\n",
    "        f[np.isnan(f)] = 0\n",
    "    \n",
    "    # each datapoint is of shape (1,16)\n",
    "    x = np.hstack(features)\n",
    "    return scaler.transform(x)\n",
    "\n",
    "def fill_random_rem_mashups(cur_list, required_size, user_id=None):\n",
    "    if user_id is not None:\n",
    "        filter_already_liked(user_id, cur_list)\n",
    "    cur_list = list(set(cur_list))\n",
    "    while len(cur_list) < required_size:\n",
    "        remainder = required_size-len(cur_list)\n",
    "        cur_list += query_db(f'SELECT mashup_id FROM mashups ORDER BY RAND() LIMIT {remainder}')\n",
    "        cur_list = list(set(cur_list))\n",
    "\n",
    "def get_pop_ids(user_id, liked_population_size, most_listened_population_size, recently_listened_population_size):\n",
    "    '''\n",
    "    Always returns list with exactly l_pop_size+m_pop_size+r_pop_size distinct db mashup ids\n",
    "    '''\n",
    "    liked_population = query_db(f'SELECT mashup_id FROM mashups_likes WHERE user_id={user_id} ORDER BY RAND() LIMIT {liked_population_size}')\n",
    "    most_listened_population = query_db(f\"SELECT mashup_id FROM mashups_likes WHERE user_id={user_id} GROUP BY mashup_id ORDER BY COUNT(`time`) DESC, RAND() LIMIT {most_listened_population_size}\")\n",
    "    recently_listened_population = query_db(f\"SELECT mashup_id FROM mashups_likes WHERE user_id={user_id} ORDER BY `time` DESC, RAND() LIMIT {recently_listened_population_size}\")\n",
    "    \n",
    "    fill_random_rem_mashups(liked_population, liked_population_size)\n",
    "    fill_random_rem_mashups(most_listened_population, most_listened_population_size)\n",
    "    fill_random_rem_mashups(recently_listened_population, recently_listened_population_size)\n",
    "        \n",
    "    return liked_population, most_listened_population, recently_listened_population\n",
    "\n",
    "def get_pop_data(liked_population_ids, most_listened_population_ids, recently_listened_population_ids):\n",
    "    liked_population = np.zeros((len(liked_population_ids),16))\n",
    "    most_listened_population = np.zeros((len(most_listened_population_ids),16))\n",
    "    recently_listened_population = np.zeros((len(recently_listened_population_ids), 16))\n",
    "    \n",
    "    # each datapoint is of shape (1,16)\n",
    "    for i in range(len(liked_population_ids)):\n",
    "        liked_population[i] = get_content_data_point(liked_population_ids[i][0])\n",
    "\n",
    "    for i in range(len(most_listened_population_ids)):\n",
    "        most_listened_population[i] = get_content_data_point(most_listened_population_ids[i][0])\n",
    "\n",
    "    for i in range(len(recently_listened_population_ids)):\n",
    "        recently_listened_population[i] = get_content_data_point(recently_listened_population_ids[i][0])\n",
    "    \n",
    "    return liked_population, most_listened_population, recently_listened_population\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T11:10:33.203333200Z",
     "start_time": "2024-07-02T11:10:33.194811100Z"
    }
   },
   "id": "f69ce57a4fd05d46",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "knn = NearestNeighbors().fit(X_scaled)\n",
    "\n",
    "def select_base_candidates(liked_population, most_listened_population, recently_listened_population, l_neighbors, m_neighbors, r_neighbors):\n",
    "    # neighbor search (candidates are returned as indices stored in ids and corresponding to elements of X provided to the KNN)\n",
    "    l_dist, l_candidates = knn.kneighbors(liked_population, l_neighbors)\n",
    "    m_dist, m_candidates = knn.kneighbors(most_listened_population, m_neighbors)\n",
    "    r_dist, r_candidates = knn.kneighbors(recently_listened_population, r_neighbors)\n",
    "\n",
    "    print(np.shape(l_dist), np.shape(m_dist), np.shape(r_dist)) # (n_population, n_neighbors)\n",
    "    # return l_dist, m_dist, r_dist, l_candidates, m_candidates, r_candidates\n",
    "    return np.concatenate((l_candidates.flatten(),m_candidates.flatten(),r_candidates.flatten()), axis=None)\n",
    "\n",
    "def select_playlist_candidates(user_id, liked_population_ids, most_listened_population_ids, recently_listened_population_ids, n_neighbors, lim):\n",
    "    # neighbor search based on playlist data (mashup-candidates are returned as indices stored in ids and corresponding to elements of X provided to the KNN)\n",
    "    playlist_ids = set()\n",
    "    # playlists including songs from the populations\n",
    "    for pop in [liked_population_ids, most_listened_population_ids, recently_listened_population_ids]:\n",
    "        for i in tqdm(pop, desc='Playlist ids from pop'):\n",
    "            for j in query_db(f'SELECT playlist_id FROM playlists_to_mashups WHERE mashup_id = {i[0]} ORDER BY RAND() LIMIT {lim}'):\n",
    "                playlist_ids.add(j[0])\n",
    "    \n",
    "    # liked playlists\n",
    "    for i in query_db(f'SELECT playlist_id FROM playlists_likes WHERE user_id = {user_id} ORDER BY RAND() LIMIT {lim}'):\n",
    "        playlist_ids.add(i[0])\n",
    "    \n",
    "    candidates = []\n",
    "    for pi in tqdm(playlist_ids, desc='Playlist based recs'):\n",
    "        mashup_ids = query_db(f'SELECT mashup_id FROM playlists_to_mashups WHERE playlist_id = {pi} ORDER BY RAND() LIMIT {lim}')\n",
    "        for mi in mashup_ids:\n",
    "            mashup_data = get_content_data_point(mi[0])\n",
    "            dist, neigh_inds = knn.kneighbors(mashup_data, n_neighbors)\n",
    "            for i in range(n_neighbors):\n",
    "                candidates.append((dist[0][i], neigh_inds[0][i]))\n",
    "                \n",
    "    return set([pair[1] for pair in sorted(candidates, key=lambda pair: pair[0])][:lim])\n",
    "\n",
    "def select_author_candidates(liked_population_ids, most_listened_population_ids, recently_listened_population_ids, n_neighbors, lim):\n",
    "    # neighbor search based on mashup author data (mashup-candidates are returned as indices stored in ids and corresponding to elements of X provided to the KNN)\n",
    "    author_ids = set()\n",
    "    # authors of mashups from the populations\n",
    "    for pop in [liked_population_ids, most_listened_population_ids, recently_listened_population_ids]:\n",
    "        for i in tqdm(pop, desc='Author ids from pop'):\n",
    "            for j in query_db(f'SELECT user_id FROM mashups_to_authors WHERE mashup_id={i[0]} ORDER BY RAND() LIMIT {lim}'):\n",
    "                author_ids.add(j[0])\n",
    "            \n",
    "    # liked mashups - already in liked_population\n",
    "            \n",
    "    candidates = []\n",
    "    for ai in tqdm(author_ids, desc='Author based recs'):\n",
    "        mashup_ids = query_db(f'SELECT mashup_id FROM mashups_to_authors WHERE user_id={ai} ORDER BY RAND() LIMIT {lim}')\n",
    "        for mi in mashup_ids:\n",
    "            mashup_data = get_content_data_point(mi[0])\n",
    "            dist, neigh_inds = knn.kneighbors(mashup_data, n_neighbors)\n",
    "            for i in range(n_neighbors):\n",
    "                candidates.append((dist[0][i], neigh_inds[0][i]))\n",
    "                \n",
    "    return set([pair[1] for pair in sorted(candidates, key=lambda pair: pair[0])][:lim])\n",
    "        \n",
    "def select_track_candidates(liked_population_ids, most_listened_population_ids, recently_listened_population_ids, n_neighbors, lim):\n",
    "    # neighbor search based on data about tracks that the mashup consists of (mashup-candidates are returned as indices stored in ids and corresponding to elements of X provided to the KNN)\n",
    "    track_ids = set()\n",
    "    # get tracks from mashups from the populations (liked mashups - already in liked_population)\n",
    "    for pop in [liked_population_ids, most_listened_population_ids, recently_listened_population_ids]:\n",
    "        for i in tqdm(pop, desc='Track ids from pop'):\n",
    "            for j in query_db(f'SELECT track_id FROM mashups_to_tracks WHERE mashup_id={i[0]} ORDER BY RAND() LIMIT {lim}'):\n",
    "                track_ids.add(j[0])\n",
    "            \n",
    "    # get the tracks' authors\n",
    "    author_ids = set()\n",
    "    for i in track_ids:\n",
    "        for j in query_db(f'SELECT author_id FROM tracks_to_authors WHERE track_id={i} ORDER BY RAND() LIMIT {lim}'):\n",
    "            author_ids.add(j[0])\n",
    "        \n",
    "    # get other tracks of the authors\n",
    "    for i in author_ids:\n",
    "        for j in query_db(f'SELECT track_id FROM tracks_to_authors WHERE author_id={i} ORDER BY RAND() LIMIT {lim}'):\n",
    "            track_ids.add(j[0])\n",
    "    \n",
    "    # recommend other mashups that include these tracks\n",
    "    candidates = []\n",
    "    for ti in tqdm(track_ids, desc='Track based recs'):\n",
    "        mashup_ids = query_db(f'SELECT mashup_id FROM mashups_to_tracks WHERE track_id={ti} ORDER BY RAND() LIMIT {lim}')\n",
    "        for mi in mashup_ids:\n",
    "            mashup_data = get_content_data_point(mi[0])\n",
    "            dist, neigh_inds = knn.kneighbors(mashup_data, n_neighbors)\n",
    "            for i in range(n_neighbors):\n",
    "                candidates.append((dist[0][i], neigh_inds[0][i]))\\\n",
    "            \n",
    "    return set([pair[1] for pair in sorted(candidates, key=lambda pair: pair[0])][:lim])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T11:25:29.469225600Z",
     "start_time": "2024-07-02T11:25:29.454710400Z"
    }
   },
   "id": "f9b9324498a48c83",
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "params_default = {\n",
    "    'liked_pop_size': 3,\n",
    "    'most_listened_pop_size': 3,\n",
    "    'recently_listened_pop_size': 3,\n",
    "    'base_l_neighb': 5,\n",
    "    'base_m_neighb': 5,\n",
    "    'base_r_neighb': 5,\n",
    "    'playlist_neighb': 3,\n",
    "    'playlist_cand_lim': 5,\n",
    "    'author_neighb': 3,\n",
    "    'author_cand_lim': 5,\n",
    "    'track_neighb': 3,\n",
    "    'track_cand_lim': 5\n",
    "    } \n",
    "\n",
    "def get_rec_list(user_id, **kwargs):\n",
    "    '''\n",
    "    :param user_id: id of a user in the db\n",
    "    :param kwargs: parameter values (if any one is not stated, it is set by default)\n",
    "    :return: list of recommended mashup db ids. number of elements: base_l_neighb + base_m_neighb + base_r_neighb + additional,\n",
    "    where additional <= playlist_cand_lim+author_cand_lim+track_cand_lim\n",
    "    '''\n",
    "    params = params_default\n",
    "    if kwargs != {}:\n",
    "        for key, value in kwargs.items():\n",
    "            params[key] = value\n",
    "           \n",
    "    population_ids = get_pop_ids(user_id, params['liked_pop_size'], params['most_listened_pop_size'], params['recently_listened_pop_size'])\n",
    "    populations = get_pop_data(*population_ids)\n",
    "\n",
    "    # get base (already filtered from already liked)\n",
    "    base_cand = select_base_candidates(*populations, params['base_l_neighb'], params['base_m_neighb'], params['base_r_neighb'])\n",
    "    \n",
    "    # get additional without already liked\n",
    "    playlist_cand = filter_already_liked(user_id, select_playlist_candidates(user_id, *population_ids, params['playlist_neighb'], params['playlist_cand_lim']))\n",
    "    author_cand = filter_already_liked(user_id, select_author_candidates(*population_ids, params['author_neighb'], params['author_cand_lim']))\n",
    "    track_cand = filter_already_liked(user_id, select_track_candidates(*population_ids, params['track_neighb'], params['track_cand_lim']))\n",
    "    \n",
    "    total_cand = list(base_cand) + playlist_cand + author_cand + track_cand\n",
    "    res = []\n",
    "    for ind in total_cand:\n",
    "        if ind >= len(ids):\n",
    "            print(f'Index {ind} out of bounds.')\n",
    "        else:\n",
    "            res.append(ids[ind][0])\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T11:27:00.409763900Z",
     "start_time": "2024-07-02T11:27:00.389569800Z"
    }
   },
   "id": "e425cfcdc905e0b3",
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5) (3, 5) (3, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Playlist ids from pop: 100%|██████████| 3/3 [00:00<00:00, 40.86it/s]\n",
      "Playlist ids from pop: 100%|██████████| 3/3 [00:00<00:00, 48.19it/s]\n",
      "Playlist ids from pop: 100%|██████████| 3/3 [00:00<00:00, 50.24it/s]\n",
      "Playlist based recs: 100%|██████████| 19/19 [00:07<00:00,  2.59it/s]\n",
      "Author ids from pop: 100%|██████████| 3/3 [00:00<00:00, 46.15it/s]\n",
      "Author ids from pop: 100%|██████████| 3/3 [00:00<00:00, 49.03it/s]\n",
      "Author ids from pop: 100%|██████████| 3/3 [00:00<00:00, 51.08it/s]\n",
      "Author based recs: 100%|██████████| 9/9 [00:03<00:00,  2.35it/s]\n",
      "Track ids from pop: 100%|██████████| 3/3 [00:00<00:00, 40.65it/s]\n",
      "Track ids from pop: 100%|██████████| 3/3 [00:00<00:00, 49.78it/s]\n",
      "Track ids from pop: 100%|██████████| 3/3 [00:00<00:00, 50.91it/s]\n",
      "Track based recs: 100%|██████████| 76/76 [00:05<00:00, 13.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[681,\n 673,\n 715,\n 709,\n 677,\n 718,\n 677,\n 709,\n 715,\n 681,\n 709,\n 677,\n 718,\n 715,\n 681,\n 709,\n 677,\n 715,\n 718,\n 681,\n 718,\n 677,\n 709,\n 715,\n 681,\n 718,\n 677,\n 709,\n 715,\n 681,\n 709,\n 715,\n 677,\n 681,\n 718,\n 718,\n 677,\n 709,\n 715,\n 681,\n 718,\n 677,\n 709,\n 715,\n 681,\n 718,\n 681,\n 681,\n 718]"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rec_list(2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T11:27:22.128145700Z",
     "start_time": "2024-07-02T11:27:03.002446400Z"
    }
   },
   "id": "b9cbbc4c1249cecb",
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Collaborative Filtering (TODO)</h1>\n",
    "Find users that liked the songs among total_candidates.<br>\n",
    "If there is not enough such users ( < num_users), then find users that listened these songs more than once.<br>\n",
    "If still not enough, select randomly.<br>\n",
    "In fact, this is an heuristic instead of KNN, which would include measuring the similarity of a given user and each other user."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d245c010f8c2d422"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_collab_users = 10\n",
    "num_collab_recs = 15"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T11:10:33.292909300Z",
     "start_time": "2024-07-02T11:10:33.266310800Z"
    }
   },
   "id": "9e45cb00b37fa1a6",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ellipsis' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[71], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m collab_users \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ind \u001B[38;5;129;01min\u001B[39;00m total_candidates:\n\u001B[0;32m      3\u001B[0m     candidate_id \u001B[38;5;241m=\u001B[39m ids[ind][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m      4\u001B[0m     response \u001B[38;5;241m=\u001B[39m query_db(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSELECT user_id FROM mashups_likes WHERE mashup_id=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcandidate_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m LIMIT \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_collab_users\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: 'ellipsis' object is not iterable"
     ]
    }
   ],
   "source": [
    "collab_users = []\n",
    "for ind in total_candidates:\n",
    "    candidate_id = ids[ind][0]\n",
    "    response = query_db(f'SELECT user_id FROM mashups_likes WHERE mashup_id={candidate_id} LIMIT {num_collab_users}')\n",
    "    print(response)\n",
    "    break\n",
    "    \n",
    "..."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T11:10:33.295900600Z",
     "start_time": "2024-07-02T11:10:33.281372600Z"
    }
   },
   "id": "39a65ff5161fd5f6",
   "execution_count": 71
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suggest to the given user such songs that they haven't listened yet, but similar users liked."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1143a550e1bd061c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "..."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T11:10:33.297900900Z",
     "start_time": "2024-07-02T11:10:33.297900900Z"
    }
   },
   "id": "a28a14e4c7922840",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
